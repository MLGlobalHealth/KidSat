{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mosaiks import get_features\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask import delayed\n",
    "from pathlib import Path\n",
    "\n",
    "# Resolves a conflict in Geopandas. Improves speed.\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "\n",
    "def apply_get_features(row):\n",
    "    lat, lon, year = row['LATNUM'], row['LONGNUM'], row['YEAR']\n",
    "    if year < 2015:\n",
    "        year = 2015\n",
    "    # Define the filename based on the centroid ID, latitude, longitude, and year\n",
    "    file_name = f\"{row['CENTROID_ID']}_{lat}_{lon}_{year}.csv\"\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_name):\n",
    "        # Read the DataFrame from the file\n",
    "        result = pd.read_csv(file_name)\n",
    "        result['year'] = year  # Ensure the year column is correct\n",
    "        return result\n",
    "    \n",
    "    result = get_features(\n",
    "        [lat],\n",
    "        [lon],\n",
    "        datetime=str(year), # or [\"2013-01-01\", \"2013-12-31\"] or ...\n",
    "        satellite_name = \"sentinel-2-l2a\", # or \"sentinel-2-l2a\",\n",
    "        image_width=10000,\n",
    "        image_resolution = 10,\n",
    "        image_bands=['B02','B03','B04'],\n",
    "        # image_bands=[\"SR_B2\", \"SR_B3\", \"SR_B4\"], # for landsat\n",
    "        model_device = \"cpu\",\n",
    "        # parallelize = True,\n",
    "        # dask_chunksize = 500\n",
    "    )\n",
    "    retry_year = year\n",
    "    result['year'] = retry_year\n",
    "    # Retry logic if the result contains all NaNs\n",
    "    while result.isna().all().all() and retry_year <= 2020:\n",
    "        retry_year += 1\n",
    "        result = get_features(lat, lon, str(retry_year))\n",
    "        result['year'] = retry_year\n",
    "    \n",
    "    # Save the result to a CSV file\n",
    "    file_name = f\"{directory}/{row['CENTROID_ID']}_{lat}_{lon}_{year}.csv\"\n",
    "    result.to_csv(file_name, index=False)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'../../survey_processing/processed_data/'\n",
    "# Create the directory if it doesn't exist\n",
    "# Specify the directory path\n",
    "directory = Path(\"mosaiks_dhs_features\")\n",
    "directory.mkdir(parents=True, exist_ok=True)\n",
    "# read data\n",
    "dhs_data = pd.read_csv(f\"{data_dir}dhs_variables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# converting to dask\n",
    "ddf = dd.from_pandas(dhs_data, npartitions=5)\n",
    "\n",
    "# Apply the function in parallel using Dask\n",
    "computed_results = ddf.apply(delayed(apply_get_features), axis=1, meta=object).compute()\n",
    "\n",
    "# Combine results into a single DataFrame\n",
    "final_results = pd.concat(computed_results.tolist(), ignore_index=True)\n",
    "\n",
    "# Save combined results to a CSV file\n",
    "final_results.to_csv(f'{data_dir}mosaiks.csv', index=False)\n",
    "\n",
    "print(\"Saved individual and combined results to CSV files\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
